{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "FTocwF3Q0VGC",
        "KiQCgjQP0-XX"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Non Dyslexic"
      ],
      "metadata": {
        "id": "1UjDAPUq0EHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install openjdk-17-jdk -y\n",
        "!update-alternatives --install /usr/bin/java java /usr/lib/jvm/java-17-openjdk-amd64/bin/java 1\n",
        "!update-alternatives --set java /usr/lib/jvm/java-17-openjdk-amd64/bin/java\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebNpdN_bA7AC",
        "outputId": "2f874a28-7c7d-41e4-950d-11afc198e298"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Connected to r2u.s\r                                                                               \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Connected to r2u.s\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Connected to r2u.s\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.82)] [3 InRelease 14.2 kB/129 k\r0% [Waiting for headers] [3 InRelease 17.1 kB/129 kB 13%] [Waiting for headers]\r0% [Waiting for headers] [3 InRelease 43.1 kB/129 kB 33%] [Waiting for headers]\r                                                                               \rGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,604 kB]\n",
            "\r0% [Waiting for headers] [3 InRelease 43.1 kB/129 kB 33%] [Waiting for headers]\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,845 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,111 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,150 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,839 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,243 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,266 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,696 kB]\n",
            "Fetched 29.0 MB in 3s (11.4 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxt-dev libxtst6 libxxf86dga1\n",
            "  openjdk-17-jdk-headless openjdk-17-jre openjdk-17-jre-headless x11-utils\n",
            "Suggested packages:\n",
            "  libxt-doc openjdk-17-demo openjdk-17-source visualvm libnss-mdns\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  | fonts-wqy-zenhei fonts-indic mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxt-dev libxtst6 libxxf86dga1 openjdk-17-jdk\n",
            "  openjdk-17-jdk-headless openjdk-17-jre openjdk-17-jre-headless x11-utils\n",
            "0 upgraded, 12 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 125 MB of archives.\n",
            "After this operation, 287 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-17-jre-headless amd64 17.0.14+7-1~22.04.1 [48.3 MB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-17-jre amd64 17.0.14+7-1~22.04.1 [232 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-17-jdk-headless amd64 17.0.14+7-1~22.04.1 [71.3 MB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-17-jdk amd64 17.0.14+7-1~22.04.1 [1,521 kB]\n",
            "Fetched 125 MB in 6s (22.0 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 12.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "(Reading database ... 126333 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../01-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../02-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../03-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../04-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../05-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../06-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../07-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package openjdk-17-jre-headless:amd64.\n",
            "Preparing to unpack .../08-openjdk-17-jre-headless_17.0.14+7-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-17-jre-headless:amd64 (17.0.14+7-1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-17-jre:amd64.\n",
            "Preparing to unpack .../09-openjdk-17-jre_17.0.14+7-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-17-jre:amd64 (17.0.14+7-1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-17-jdk-headless:amd64.\n",
            "Preparing to unpack .../10-openjdk-17-jdk-headless_17.0.14+7-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-17-jdk-headless:amd64 (17.0.14+7-1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-17-jdk:amd64.\n",
            "Preparing to unpack .../11-openjdk-17-jdk_17.0.14+7-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-17-jdk:amd64 (17.0.14+7-1~22.04.1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up openjdk-17-jre-headless:amd64 (17.0.14+7-1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jpackage to provide /usr/bin/jpackage (jpackage) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up openjdk-17-jre:amd64 (17.0.14+7-1~22.04.1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up openjdk-17-jdk-headless:amd64 (17.0.14+7-1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jar to provide /usr/bin/jar (jar) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jarsigner to provide /usr/bin/jarsigner (jarsigner) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/javadoc to provide /usr/bin/javadoc (javadoc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/javap to provide /usr/bin/javap (javap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jcmd to provide /usr/bin/jcmd (jcmd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jdb to provide /usr/bin/jdb (jdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jdeprscan to provide /usr/bin/jdeprscan (jdeprscan) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jdeps to provide /usr/bin/jdeps (jdeps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jfr to provide /usr/bin/jfr (jfr) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jimage to provide /usr/bin/jimage (jimage) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jinfo to provide /usr/bin/jinfo (jinfo) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jlink to provide /usr/bin/jlink (jlink) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jmap to provide /usr/bin/jmap (jmap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jmod to provide /usr/bin/jmod (jmod) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jrunscript to provide /usr/bin/jrunscript (jrunscript) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jshell to provide /usr/bin/jshell (jshell) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jstack to provide /usr/bin/jstack (jstack) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jstat to provide /usr/bin/jstat (jstat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jstatd to provide /usr/bin/jstatd (jstatd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/serialver to provide /usr/bin/serialver (serialver) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jhsdb to provide /usr/bin/jhsdb (jhsdb) in auto mode\n",
            "Setting up openjdk-17-jdk:amd64 (17.0.14+7-1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/java to provide /usr/bin/java (java) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "zip_path = \"non_dyslexic.zip\"\n",
        "extract_path = \"non_dyslexic_images\"\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "print(f\"✅ Unzipped to folder: {extract_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zvvv8VtCJhvk",
        "outputId": "aec7eb93-144a-4b43-d007-24a486f5a660"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Unzipped to folder: non_dyslexic_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrAaDZDXIly2",
        "outputId": "99210abb-0ae6-4d2d-934d-9e9a8ed96a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: language-tool-python in /usr/local/lib/python3.11/dist-packages (2.9.3)\n",
            "Requirement already satisfied: eng_to_ipa in /usr/local/lib/python3.11/dist-packages (0.0.2)\n",
            "Requirement already satisfied: abydos in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: azure-cognitiveservices-vision-computervision in /usr/local/lib/python3.11/dist-packages (0.9.1)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (5.9.5)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (0.10.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from abydos) (2.0.2)\n",
            "Requirement already satisfied: deprecation in /usr/local/lib/python3.11/dist-packages (from abydos) (2.1.0)\n",
            "Requirement already satisfied: msrest>=0.6.21 in /usr/local/lib/python3.11/dist-packages (from azure-cognitiveservices-vision-computervision) (0.7.1)\n",
            "Requirement already satisfied: azure-common~=1.1 in /usr/local/lib/python3.11/dist-packages (from azure-cognitiveservices-vision-computervision) (1.1.28)\n",
            "Requirement already satisfied: azure-core>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from msrest>=0.6.21->azure-cognitiveservices-vision-computervision) (1.33.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from msrest>=0.6.21->azure-cognitiveservices-vision-computervision) (2025.1.31)\n",
            "Requirement already satisfied: isodate>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from msrest>=0.6.21->azure-cognitiveservices-vision-computervision) (0.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from msrest>=0.6.21->azure-cognitiveservices-vision-computervision) (2.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (2.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from deprecation->abydos) (24.2)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.24.0->msrest>=0.6.21->azure-cognitiveservices-vision-computervision) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.24.0->msrest>=0.6.21->azure-cognitiveservices-vision-computervision) (4.13.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-cognitiveservices-vision-computervision) (3.2.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading LanguageTool latest: 100%|██████████| 252M/252M [00:16<00:00, 14.9MB/s]\n",
            "INFO:language_tool_python.download_lt:Unzipping /tmp/tmp8pjw5_0o.zip to /root/.cache/language_tool_python.\n",
            "INFO:language_tool_python.download_lt:Downloaded https://internal1.languagetool.org/snapshots/LanguageTool-latest-snapshot.zip to /root/.cache/language_tool_python.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🖼️ Processing 43.jpg...\n",
            "🖼️ Processing 48.jpg...\n",
            "🖼️ Processing 42.jpg...\n",
            "🖼️ Processing 20.jpg...\n",
            "🖼️ Processing 1.jpg...\n",
            "🖼️ Processing 31.jpg...\n",
            "🖼️ Processing 27.jpg...\n",
            "🖼️ Processing 29.jpg...\n",
            "🖼️ Processing 26.jpg...\n",
            "🖼️ Processing 44.jpg...\n",
            "🖼️ Processing 28.jpg...\n",
            "🖼️ Processing 8.jpg...\n",
            "🖼️ Processing 45.jpg...\n",
            "🖼️ Processing 11.jpg...\n",
            "🖼️ Processing 15.jpg...\n",
            "🖼️ Processing 35.jpg...\n",
            "❌ Error processing non_dyslexic_images/non_dyslexic/35.jpg: Operation returned an invalid status code 'Too Many Requests'\n",
            "🖼️ Processing 32.jpg...\n",
            "🖼️ Processing 24.jpg...\n",
            "❌ Error processing non_dyslexic_images/non_dyslexic/24.jpg: Operation returned an invalid status code 'Too Many Requests'\n",
            "🖼️ Processing 46.jpg...\n",
            "🖼️ Processing 18.jpg...\n",
            "🖼️ Processing 5.jpg...\n",
            "🖼️ Processing 30.jpg...\n",
            "🖼️ Processing 19.jpg...\n",
            "🖼️ Processing 10.jpg...\n",
            "❌ Error processing non_dyslexic_images/non_dyslexic/10.jpg: Operation returned an invalid status code 'Too Many Requests'\n",
            "🖼️ Processing 39.jpg...\n",
            "❌ Error processing non_dyslexic_images/non_dyslexic/39.jpg: Operation returned an invalid status code 'Too Many Requests'\n",
            "🖼️ Processing 3.jpg...\n",
            "❌ Error processing non_dyslexic_images/non_dyslexic/3.jpg: Operation returned an invalid status code 'Too Many Requests'\n",
            "🖼️ Processing 37.jpg...\n",
            "❌ Error processing non_dyslexic_images/non_dyslexic/37.jpg: Operation returned an invalid status code 'Too Many Requests'\n",
            "🖼️ Processing 50.jpg...\n",
            "❌ Error processing non_dyslexic_images/non_dyslexic/50.jpg: Operation returned an invalid status code 'Too Many Requests'\n",
            "🖼️ Processing 25.jpg...\n",
            "❌ Error processing non_dyslexic_images/non_dyslexic/25.jpg: Operation returned an invalid status code 'Too Many Requests'\n",
            "🖼️ Processing 41.jpg...\n",
            "🖼️ Processing 13.jpg...\n",
            "🖼️ Processing 40.jpg...\n",
            "❌ Error processing non_dyslexic_images/non_dyslexic/40.jpg: Operation returned an invalid status code 'Too Many Requests'\n",
            "🖼️ Processing 14.jpg...\n",
            "🖼️ Processing 17.jpg...\n",
            "🖼️ Processing 21.jpg...\n",
            "🖼️ Processing 23.jpg...\n",
            "❌ Error processing non_dyslexic_images/non_dyslexic/23.jpg: Operation returned an invalid status code 'Too Many Requests'\n",
            "🖼️ Processing 7.jpg...\n",
            "🖼️ Processing 38.jpg...\n",
            "🖼️ Processing 36.jpg...\n",
            "🖼️ Processing 9.jpg...\n",
            "🖼️ Processing 12.jpg...\n",
            "❌ Error processing non_dyslexic_images/non_dyslexic/12.jpg: Operation returned an invalid status code 'Too Many Requests'\n",
            "🖼️ Processing 49.jpg...\n",
            "🖼️ Processing 47.jpg...\n",
            "🖼️ Processing 33.jpg...\n",
            "🖼️ Processing 34.jpg...\n",
            "🖼️ Processing 16.jpg...\n",
            "🖼️ Processing 6.jpg...\n",
            "🖼️ Processing 22.jpg...\n",
            "🖼️ Processing 2.jpg...\n",
            "❌ Error processing non_dyslexic_images/non_dyslexic/2.jpg: Operation returned an invalid status code 'Too Many Requests'\n",
            "🖼️ Processing 4.jpg...\n",
            "✅ Done. Saved as non_dyslexic_data.csv\n"
          ]
        }
      ],
      "source": [
        "!pip install textblob language-tool-python eng_to_ipa abydos azure-cognitiveservices-vision-computervision\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import language_tool_python\n",
        "import eng_to_ipa as ipa\n",
        "from abydos.phonetic import Soundex, Metaphone, Caverphone, NYSIIS\n",
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "\n",
        "AZURE_ENDPOINT = \"https://dyslexiazd.cognitiveservices.azure.com/\"\n",
        "AZURE_KEY = \"5CKFAIsdDlK3y6X1NoWVsUNNq7ZGxNUYCYDc67Ix92ipT5OQa96uJQQJ99BDACYeBjFXJ3w3AAAFACOGlflR\"\n",
        "\n",
        "computervision_client = ComputerVisionClient(\n",
        "    AZURE_ENDPOINT, CognitiveServicesCredentials(AZURE_KEY)\n",
        ")\n",
        "tool = language_tool_python.LanguageTool('en-US')\n",
        "def levenshtein(s1, s2):\n",
        "    if len(s1) < len(s2):\n",
        "        return levenshtein(s2, s1)\n",
        "    if len(s2) == 0:\n",
        "        return len(s1)\n",
        "    previous_row = range(len(s2) + 1)\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "    return previous_row[-1]\n",
        "\n",
        "# OCR\n",
        "def image_to_text(path):\n",
        "    with open(path, \"rb\") as image_stream:\n",
        "        image = image_stream.read()\n",
        "    read_response = computervision_client.read_in_stream(io.BytesIO(image), raw=True)\n",
        "    read_operation_location = read_response.headers[\"Operation-Location\"]\n",
        "    operation_id = read_operation_location.split(\"/\")[-1]\n",
        "\n",
        "    while True:\n",
        "        result = computervision_client.get_read_result(operation_id)\n",
        "        if result.status not in ['notStarted', 'running']:\n",
        "            break\n",
        "        time.sleep(1)\n",
        "\n",
        "    extracted_text = []\n",
        "    if result.status == OperationStatusCodes.succeeded:\n",
        "        for page in result.analyze_result.read_results:\n",
        "            for line in page.lines:\n",
        "                extracted_text.append(line.text)\n",
        "    return \" \".join(extracted_text)\n",
        "\n",
        "# Features\n",
        "def spelling_accuracy(extracted_text):\n",
        "    spell_corrected = TextBlob(extracted_text).correct()\n",
        "    return ((len(extracted_text) - levenshtein(extracted_text, spell_corrected)) / (len(extracted_text) + 1)) * 100\n",
        "\n",
        "def grammatical_accuracy(text):\n",
        "    matches = tool.check(text)\n",
        "    total_words = len(text.split())\n",
        "    errors = len(matches)\n",
        "    accuracy = (1 - errors / (total_words + 1)) * 100\n",
        "    return accuracy\n",
        "\n",
        "def percentage_of_corrections(text, matches=None):\n",
        "    if matches is None:\n",
        "        matches = tool.check(text)\n",
        "    return len(matches) / (len(text.split()) + 1) * 100\n",
        "\n",
        "def percentage_of_phonetic_accuraccy(extracted_text: str):\n",
        "    soundex = Soundex()\n",
        "    metaphone = Metaphone()\n",
        "    caverphone = Caverphone()\n",
        "    nysiis = NYSIIS()\n",
        "    spell_corrected = TextBlob(extracted_text).correct()\n",
        "\n",
        "    extracted_text_list = extracted_text.split(\" \")\n",
        "    extracted_phonetics_soundex = [soundex.encode(w) for w in extracted_text_list]\n",
        "    extracted_phonetics_metaphone = [metaphone.encode(w) for w in extracted_text_list]\n",
        "    extracted_phonetics_caverphone = [caverphone.encode(w) for w in extracted_text_list]\n",
        "    extracted_phonetics_nysiis = [nysiis.encode(w) for w in extracted_text_list]\n",
        "\n",
        "    spell_corrected_list = spell_corrected.split(\" \")\n",
        "    spell_corrected_phonetics_soundex = [soundex.encode(w) for w in spell_corrected_list]\n",
        "    spell_corrected_phonetics_metaphone = [metaphone.encode(w) for w in spell_corrected_list]\n",
        "    spell_corrected_phonetics_caverphone = [caverphone.encode(w) for w in spell_corrected_list]\n",
        "    spell_corrected_phonetics_nysiis = [nysiis.encode(w) for w in spell_corrected_list]\n",
        "\n",
        "    soundex_score = (len(\" \".join(extracted_phonetics_soundex)) - levenshtein(\" \".join(extracted_phonetics_soundex), \" \".join(spell_corrected_phonetics_soundex))) / (len(\" \".join(extracted_phonetics_soundex)) + 1)\n",
        "    metaphone_score = (len(\" \".join(extracted_phonetics_metaphone)) - levenshtein(\" \".join(extracted_phonetics_metaphone), \" \".join(spell_corrected_phonetics_metaphone))) / (len(\" \".join(extracted_phonetics_metaphone)) + 1)\n",
        "    caverphone_score = (len(\" \".join(extracted_phonetics_caverphone)) - levenshtein(\" \".join(extracted_phonetics_caverphone), \" \".join(spell_corrected_phonetics_caverphone))) / (len(\" \".join(extracted_phonetics_caverphone)) + 1)\n",
        "    nysiis_score = (len(\" \".join(extracted_phonetics_nysiis)) - levenshtein(\" \".join(extracted_phonetics_nysiis), \" \".join(spell_corrected_phonetics_nysiis))) / (len(\" \".join(extracted_phonetics_nysiis)) + 1)\n",
        "\n",
        "    return (0.5 * caverphone_score + 0.2 * soundex_score + 0.2 * metaphone_score + 0.1 * nysiis_score) * 100\n",
        "\n",
        "# Full feature extraction\n",
        "def get_feature_array(path: str):\n",
        "    try:\n",
        "        extracted_text = image_to_text(path)\n",
        "        features = [\n",
        "            spelling_accuracy(extracted_text),\n",
        "            grammatical_accuracy(extracted_text),\n",
        "            percentage_of_corrections(extracted_text),\n",
        "            percentage_of_phonetic_accuraccy(extracted_text)\n",
        "        ]\n",
        "        return features\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing {path}: {e}\")\n",
        "        return [None, None, None, None]\n",
        "    time.sleep(2)\n",
        "\n",
        "# Process all images and save to CSV\n",
        "folder = \"non_dyslexic_images/non_dyslexic\"  # <-- folder where your images are uploaded\n",
        "label = 0  # Non-dyslexic\n",
        "data = []\n",
        "\n",
        "for image_name in os.listdir(folder):\n",
        "    path = os.path.join(folder, image_name)\n",
        "    print(f\"🖼️ Processing {image_name}...\")\n",
        "    features = get_feature_array(path)\n",
        "    if None not in features:\n",
        "        features.append(label)\n",
        "        data.append(features)\n",
        "    time.sleep(2)\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\n",
        "    \"spelling_accuracy\",\n",
        "    \"grammatical_accuracy\",\n",
        "    \"percentage_of_corrections\",\n",
        "    \"percentage_of_phonetic_accuraccy\",\n",
        "    \"presence_of_dyslexia\"\n",
        "])\n",
        "\n",
        "df.to_csv(\"non_dyslexic_data.csv\", index=False)\n",
        "print(\"✅ Done. Saved as non_dyslexic_data.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dyslexic"
      ],
      "metadata": {
        "id": "fYZ8BPN-1Rz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"dyslexic.zip\"\n",
        "extract_path = \"dyslexic_images\"\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"✅ Unzipped to folder: {extract_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9GeU9LUL-iR",
        "outputId": "6685946d-06ac-46a8-dd1d-6051ec44e295"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Unzipped to folder: dyslexic_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob language-tool-python eng_to_ipa abydos azure-cognitiveservices-vision-computervision\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import language_tool_python\n",
        "import eng_to_ipa as ipa\n",
        "from abydos.phonetic import Soundex, Metaphone, Caverphone, NYSIIS\n",
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "AZURE_ENDPOINT = \"https://dyslexiazd.cognitiveservices.azure.com/\"\n",
        "AZURE_KEY = \"5CKFAIsdDlK3y6X1NoWVsUNNq7ZGxNUYCYDc67Ix92ipT5OQa96uJQQJ99BDACYeBjFXJ3w3AAAFACOGlflR\"\n",
        "computervision_client = ComputerVisionClient(\n",
        "    AZURE_ENDPOINT, CognitiveServicesCredentials(AZURE_KEY)\n",
        ")\n",
        "tool = language_tool_python.LanguageTool('en-US')\n",
        "\n",
        "def levenshtein(s1, s2):\n",
        "    if len(s1) < len(s2):\n",
        "        return levenshtein(s2, s1)\n",
        "    if len(s2) == 0:\n",
        "        return len(s1)\n",
        "    previous_row = range(len(s2) + 1)\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "    return previous_row[-1]\n",
        "\n",
        "# OCR\n",
        "def image_to_text(path):\n",
        "    with open(path, \"rb\") as image_stream:\n",
        "        image = image_stream.read()\n",
        "    read_response = computervision_client.read_in_stream(io.BytesIO(image), raw=True)\n",
        "    read_operation_location = read_response.headers[\"Operation-Location\"]\n",
        "    operation_id = read_operation_location.split(\"/\")[-1]\n",
        "\n",
        "    while True:\n",
        "        result = computervision_client.get_read_result(operation_id)\n",
        "        if result.status not in ['notStarted', 'running']:\n",
        "            break\n",
        "        time.sleep(1)\n",
        "\n",
        "    extracted_text = []\n",
        "    if result.status == OperationStatusCodes.succeeded:\n",
        "        for page in result.analyze_result.read_results:\n",
        "            for line in page.lines:\n",
        "                extracted_text.append(line.text)\n",
        "    return \" \".join(extracted_text)\n",
        "\n",
        "# Features\n",
        "def spelling_accuracy(extracted_text):\n",
        "    spell_corrected = TextBlob(extracted_text).correct()\n",
        "    return ((len(extracted_text) - levenshtein(extracted_text, spell_corrected)) / (len(extracted_text) + 1)) * 100\n",
        "\n",
        "def grammatical_accuracy(text):\n",
        "    matches = tool.check(text)\n",
        "    total_words = len(text.split())\n",
        "    errors = len(matches)\n",
        "    accuracy = (1 - errors / (total_words + 1)) * 100\n",
        "    return accuracy\n",
        "\n",
        "def percentage_of_corrections(text, matches=None):\n",
        "    if matches is None:\n",
        "        matches = tool.check(text)\n",
        "    return len(matches) / (len(text.split()) + 1) * 100\n",
        "\n",
        "def percentage_of_phonetic_accuraccy(extracted_text: str):\n",
        "    soundex = Soundex()\n",
        "    metaphone = Metaphone()\n",
        "    caverphone = Caverphone()\n",
        "    nysiis = NYSIIS()\n",
        "    spell_corrected = TextBlob(extracted_text).correct()\n",
        "\n",
        "    extracted_text_list = extracted_text.split(\" \")\n",
        "    extracted_phonetics_soundex = [soundex.encode(w) for w in extracted_text_list]\n",
        "    extracted_phonetics_metaphone = [metaphone.encode(w) for w in extracted_text_list]\n",
        "    extracted_phonetics_caverphone = [caverphone.encode(w) for w in extracted_text_list]\n",
        "    extracted_phonetics_nysiis = [nysiis.encode(w) for w in extracted_text_list]\n",
        "\n",
        "    spell_corrected_list = spell_corrected.split(\" \")\n",
        "    spell_corrected_phonetics_soundex = [soundex.encode(w) for w in spell_corrected_list]\n",
        "    spell_corrected_phonetics_metaphone = [metaphone.encode(w) for w in spell_corrected_list]\n",
        "    spell_corrected_phonetics_caverphone = [caverphone.encode(w) for w in spell_corrected_list]\n",
        "    spell_corrected_phonetics_nysiis = [nysiis.encode(w) for w in spell_corrected_list]\n",
        "\n",
        "    soundex_score = (len(\" \".join(extracted_phonetics_soundex)) - levenshtein(\" \".join(extracted_phonetics_soundex), \" \".join(spell_corrected_phonetics_soundex))) / (len(\" \".join(extracted_phonetics_soundex)) + 1)\n",
        "    metaphone_score = (len(\" \".join(extracted_phonetics_metaphone)) - levenshtein(\" \".join(extracted_phonetics_metaphone), \" \".join(spell_corrected_phonetics_metaphone))) / (len(\" \".join(extracted_phonetics_metaphone)) + 1)\n",
        "    caverphone_score = (len(\" \".join(extracted_phonetics_caverphone)) - levenshtein(\" \".join(extracted_phonetics_caverphone), \" \".join(spell_corrected_phonetics_caverphone))) / (len(\" \".join(extracted_phonetics_caverphone)) + 1)\n",
        "    nysiis_score = (len(\" \".join(extracted_phonetics_nysiis)) - levenshtein(\" \".join(extracted_phonetics_nysiis), \" \".join(spell_corrected_phonetics_nysiis))) / (len(\" \".join(extracted_phonetics_nysiis)) + 1)\n",
        "\n",
        "    return (0.5 * caverphone_score + 0.2 * soundex_score + 0.2 * metaphone_score + 0.1 * nysiis_score) * 100\n",
        "\n",
        "# Full feature extraction\n",
        "def get_feature_array(path: str):\n",
        "    try:\n",
        "        extracted_text = image_to_text(path)\n",
        "        features = [\n",
        "            spelling_accuracy(extracted_text),\n",
        "            grammatical_accuracy(extracted_text),\n",
        "            percentage_of_corrections(extracted_text),\n",
        "            percentage_of_phonetic_accuraccy(extracted_text)\n",
        "        ]\n",
        "        return features\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing {path}: {e}\")\n",
        "        return [None, None, None, None]\n",
        "\n",
        "\n",
        "folder = \"dyslexic_images/dyslexic\"  # <-- folder where dyslexic images are uploaded\n",
        "label = 1  # Dyslexic\n",
        "data = []\n",
        "\n",
        "for image_name in os.listdir(folder):\n",
        "    path = os.path.join(folder, image_name)\n",
        "    print(f\"🖼️ Processing {image_name}...\")\n",
        "    features = get_feature_array(path)\n",
        "    if None not in features:\n",
        "        features.append(label)\n",
        "        data.append(features)\n",
        "    time.sleep(3)\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\n",
        "    \"spelling_accuracy\",\n",
        "    \"grammatical_accuracy\",\n",
        "    \"percentage_of_corrections\",\n",
        "    \"percentage_of_phonetic_accuraccy\",\n",
        "    \"presence_of_dyslexia\"\n",
        "])\n",
        "\n",
        "df.to_csv(\"dyslexic_data.csv\", index=False)\n",
        "print(\"✅ Done. Saved as dyslexic_data.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZbzsH1CLwRu",
        "outputId": "b2dbe69d-5529-4da0-a83f-83cf0f0b70b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: language-tool-python in /usr/local/lib/python3.11/dist-packages (2.9.3)\n",
            "Requirement already satisfied: eng_to_ipa in /usr/local/lib/python3.11/dist-packages (0.0.2)\n",
            "Requirement already satisfied: abydos in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: azure-cognitiveservices-vision-computervision in /usr/local/lib/python3.11/dist-packages (0.9.1)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (5.9.5)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (0.10.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from abydos) (2.0.2)\n",
            "Requirement already satisfied: deprecation in /usr/local/lib/python3.11/dist-packages (from abydos) (2.1.0)\n",
            "Requirement already satisfied: msrest>=0.6.21 in /usr/local/lib/python3.11/dist-packages (from azure-cognitiveservices-vision-computervision) (0.7.1)\n",
            "Requirement already satisfied: azure-common~=1.1 in /usr/local/lib/python3.11/dist-packages (from azure-cognitiveservices-vision-computervision) (1.1.28)\n",
            "Requirement already satisfied: azure-core>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from msrest>=0.6.21->azure-cognitiveservices-vision-computervision) (1.33.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from msrest>=0.6.21->azure-cognitiveservices-vision-computervision) (2025.1.31)\n",
            "Requirement already satisfied: isodate>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from msrest>=0.6.21->azure-cognitiveservices-vision-computervision) (0.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from msrest>=0.6.21->azure-cognitiveservices-vision-computervision) (2.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (2.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from deprecation->abydos) (24.2)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.24.0->msrest>=0.6.21->azure-cognitiveservices-vision-computervision) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.24.0->msrest>=0.6.21->azure-cognitiveservices-vision-computervision) (4.13.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-cognitiveservices-vision-computervision) (3.2.2)\n",
            "🖼️ Processing 43.jpg...\n",
            "🖼️ Processing 48.jpg...\n",
            "🖼️ Processing 42.jpg...\n",
            "🖼️ Processing 20.jpg...\n",
            "🖼️ Processing 1.jpg...\n",
            "🖼️ Processing 31.jpg...\n",
            "🖼️ Processing 27.jpg...\n",
            "🖼️ Processing 29.jpg...\n",
            "🖼️ Processing 26.jpg...\n",
            "🖼️ Processing 44.jpg...\n",
            "🖼️ Processing 28.jpg...\n",
            "🖼️ Processing 8.jpg...\n",
            "🖼️ Processing 45.jpg...\n",
            "🖼️ Processing 11.jpg...\n",
            "🖼️ Processing 15.jpg...\n",
            "❌ Error processing dyslexic_images/dyslexic/15.jpg: Operation returned an invalid status code 'Too Many Requests'\n",
            "🖼️ Processing 35.jpg...\n",
            "🖼️ Processing 32.jpg...\n",
            "🖼️ Processing 24.jpg...\n",
            "🖼️ Processing 46.jpg...\n",
            "❌ Error processing dyslexic_images/dyslexic/46.jpg: Operation returned an invalid status code 'Too Many Requests'\n",
            "🖼️ Processing 18.jpg...\n",
            "🖼️ Processing 5.jpg...\n",
            "🖼️ Processing 30.jpg...\n",
            "🖼️ Processing 19.jpg...\n",
            "🖼️ Processing 10.jpg...\n",
            "🖼️ Processing 39.jpg...\n",
            "❌ Error processing dyslexic_images/dyslexic/39.jpg: Operation returned an invalid status code 'Too Many Requests'\n",
            "🖼️ Processing 3.jpg...\n",
            "🖼️ Processing 37.jpg...\n",
            "🖼️ Processing 50.jpg...\n",
            "🖼️ Processing 25.jpg...\n",
            "🖼️ Processing 41.jpg...\n",
            "🖼️ Processing 13.jpg...\n",
            "🖼️ Processing 40.jpg...\n",
            "🖼️ Processing 14.jpg...\n",
            "🖼️ Processing 17.jpg...\n",
            "🖼️ Processing 21.jpg...\n",
            "🖼️ Processing 23.jpg...\n",
            "🖼️ Processing 7.jpg...\n",
            "🖼️ Processing 38.jpg...\n",
            "🖼️ Processing 36.jpg...\n",
            "🖼️ Processing 9.jpg...\n",
            "🖼️ Processing 12.jpg...\n",
            "🖼️ Processing 49.jpg...\n",
            "🖼️ Processing 47.jpg...\n",
            "🖼️ Processing 33.jpg...\n",
            "🖼️ Processing 34.jpg...\n",
            "🖼️ Processing 16.jpg...\n",
            "🖼️ Processing 6.jpg...\n",
            "🖼️ Processing 22.jpg...\n",
            "🖼️ Processing 2.jpg...\n",
            "🖼️ Processing 4.jpg...\n",
            "✅ Done. Saved as dyslexic_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merging"
      ],
      "metadata": {
        "id": "FTocwF3Q0VGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_dyslexic = pd.read_csv(\"dyslexic_data.csv\")\n",
        "df_non_dyslexic = pd.read_csv(\"non_dyslexic_data.csv\")\n",
        "\n",
        "df_combined = pd.concat([df_dyslexic, df_non_dyslexic], ignore_index=True)\n",
        "\n",
        "print(\"🔍 Combined Dataset Preview:\")\n",
        "print(df_combined.head())\n",
        "print(\"\\n🚨 Missing Values:\")\n",
        "print(df_combined.isnull().sum())\n",
        "\n",
        "df_combined.to_csv(\"combined_dataset.csv\", index=False)\n",
        "print(\"\\n✅ Saved merged dataset as combined_dataset.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR27fI7hNTmt",
        "outputId": "52a41cff-e63d-43bf-9ad0-dbd552af8afd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Combined Dataset Preview:\n",
            "   spelling_accuracy  grammatical_accuracy  percentage_of_corrections  \\\n",
            "0          97.389034             84.507042                  15.492958   \n",
            "1          90.109890             68.181818                  31.818182   \n",
            "2          93.886463             83.673469                  16.326531   \n",
            "3          90.243902             58.181818                  41.818182   \n",
            "4          90.000000             79.166667                  20.833333   \n",
            "\n",
            "   percentage_of_phonetic_accuraccy  presence_of_dyslexia  \n",
            "0                         97.990193                     1  \n",
            "1                         93.058412                     1  \n",
            "2                         95.795916                     1  \n",
            "3                         93.017022                     1  \n",
            "4                         92.025391                     1  \n",
            "\n",
            "🚨 Missing Values:\n",
            "spelling_accuracy                   0\n",
            "grammatical_accuracy                0\n",
            "percentage_of_corrections           0\n",
            "percentage_of_phonetic_accuraccy    0\n",
            "presence_of_dyslexia                0\n",
            "dtype: int64\n",
            "\n",
            "✅ Saved merged dataset as combined_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "75qvMdZq08wD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rule Extraction"
      ],
      "metadata": {
        "id": "KiQCgjQP0-XX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load and clean\n",
        "df = pd.read_csv(\"combined_dataset.csv\")\n",
        "df_clean = df.dropna()\n",
        "\n",
        "X = df_clean.drop(columns=[\"presence_of_dyslexia\"])\n",
        "y = df_clean[\"presence_of_dyslexia\"]\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train small interpretable decision tree\n",
        "tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Print decision rules\n",
        "rules = export_text(tree, feature_names=list(X.columns))\n",
        "print(\"🌳 Simple interpretable threshold logic:\\n\")\n",
        "print(rules)\n"
      ],
      "metadata": {
        "id": "bbSXojfqOHsO",
        "outputId": "3c08281e-8fb2-469a-8dc3-af767b6c36c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌳 Simple interpretable threshold logic:\n",
            "\n",
            "|--- spelling_accuracy <= 96.25\n",
            "|   |--- class: 1\n",
            "|--- spelling_accuracy >  96.25\n",
            "|   |--- spelling_accuracy <= 97.88\n",
            "|   |   |--- grammatical_accuracy <= 90.52\n",
            "|   |   |   |--- class: 0\n",
            "|   |   |--- grammatical_accuracy >  90.52\n",
            "|   |   |   |--- class: 1\n",
            "|   |--- spelling_accuracy >  97.88\n",
            "|   |   |--- class: 0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rule Extraction using Decision Tree\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "\n",
        "# Load the datasets\n",
        "df1 = pd.read_csv(\"non_dyslexic_data.csv\")\n",
        "df2 = pd.read_csv(\"dyslexic_data.csv\")\n",
        "df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(columns=[\"presence_of_dyslexia\"])\n",
        "y = df[\"presence_of_dyslexia\"]\n",
        "\n",
        "# Train a Decision Tree\n",
        "model = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Display the rules\n",
        "features = list(X.columns)\n",
        "tree_rules = export_text(model, feature_names=features)\n",
        "print(\"🌳 Simple interpretable threshold logic:\\n\")\n",
        "print(tree_rules)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFoBzZ52Fm4N",
        "outputId": "7c586a4d-5551-4279-c2f5-e554beff002e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌳 Simple interpretable threshold logic:\n",
            "\n",
            "|--- spelling_accuracy <= 96.38\n",
            "|   |--- class: 1\n",
            "|--- spelling_accuracy >  96.38\n",
            "|   |--- spelling_accuracy <= 97.88\n",
            "|   |   |--- grammatical_accuracy <= 90.52\n",
            "|   |   |   |--- class: 0\n",
            "|   |   |--- grammatical_accuracy >  90.52\n",
            "|   |   |   |--- class: 1\n",
            "|   |--- spelling_accuracy >  97.88\n",
            "|   |   |--- class: 0\n",
            "\n"
          ]
        }
      ]
    }
  ]
}